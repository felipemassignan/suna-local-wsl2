# SUNA Local WSL2 Setup

Uma implementa√ß√£o completamente local do framework de agente de IA SUNA, adaptada especificamente para rodar no WSL2 do Windows. Este sistema substitui todas as depend√™ncias de nuvem por alternativas locais, utilizando o modelo Mistral 7B via llama.cpp e banco de dados SQLite.

## üéØ Caracter√≠sticas Principais

- **Opera√ß√£o 100% offline** - sem depend√™ncias de APIs externas
- **Otimizado para WSL2** - configura√ß√£o espec√≠fica para Windows Subsystem for Linux 2
- **Modelo Mistral 7B local** via llama.cpp com API compat√≠vel com OpenAI
- **Banco de dados SQLite** substituindo Supabase
- **Autentica√ß√£o local** sem necessidade de servi√ßos externos
- **Vector store local** usando FAISS para busca sem√¢ntica
- **Interface web completa** com frontend Next.js
- **Ferramentas locais** substituindo APIs externas (busca, gera√ß√£o de imagem, etc.)

## üìã Requisitos do Sistema

### Hardware M√≠nimo
- **CPU**: 4 cores (recomendado 8+ cores)
- **RAM**: 16GB (recomendado 32GB)
- **Armazenamento**: 20GB livres
- **GPU**: N√£o necess√°ria (otimizado para CPU)

### Software
- **Windows 10/11** com WSL2 habilitado
- **Ubuntu 20.04+** no WSL2
- **Python 3.8+**
- **Node.js 18+**
- **Git**

## üöÄ Instala√ß√£o R√°pida

### 1. Preparar o WSL2

```bash
# No PowerShell como Administrador
wsl --install Ubuntu
wsl --set-default-version 2
```

### 2. Clonar o Reposit√≥rio

```bash
# No terminal WSL2
git clone <este-repositorio>
cd suna-wsl2-setup
```

### 3. Executar Instala√ß√£o

```bash
# Executar como usu√°rio normal (n√£o root)
./install-wsl2.sh
```

### 4. Iniciar o Sistema

```bash
cd ~/suna-local
./start-suna.sh
```

### 5. Acessar a Interface

- **Frontend**: http://localhost:3000
- **API Backend**: http://localhost:8080
- **API Llama**: http://localhost:8000

## üìÅ Estrutura do Projeto

```
~/suna-local/
‚îú‚îÄ‚îÄ backend/                 # Backend FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ agent/              # L√≥gica do agente
‚îÇ   ‚îú‚îÄ‚îÄ services/           # Servi√ßos locais
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Utilit√°rios
‚îú‚îÄ‚îÄ frontend/               # Frontend Next.js
‚îú‚îÄ‚îÄ models/                 # Modelos de IA
‚îÇ   ‚îî‚îÄ‚îÄ mistral-7b-instruct-v0.2.Q4_K_M.gguf
‚îú‚îÄ‚îÄ data/                   # Dados locais
‚îÇ   ‚îú‚îÄ‚îÄ sqlite/            # Banco SQLite
‚îÇ   ‚îú‚îÄ‚îÄ vector_store/      # Vector store FAISS
‚îÇ   ‚îî‚îÄ‚îÄ logs/              # Logs do sistema
‚îú‚îÄ‚îÄ venv/                   # Ambiente virtual Python
‚îî‚îÄ‚îÄ scripts/                # Scripts de controle
```

## üîß Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente

O sistema usa as seguintes vari√°veis de ambiente principais:

```bash
# Backend (.env)
ENV_MODE=LOCAL
OPENAI_API_KEY=sk-dummy-key
OPENAI_API_BASE=http://localhost:8000/v1
SQLITE_DB_PATH=./data/sqlite/suna.db
VECTOR_STORE_PATH=./data/vector_store
REDIS_URL=redis://localhost:6379

# Frontend (.env.local)
NEXT_PUBLIC_API_URL=http://localhost:8080
ENV_MODE=LOCAL
```

### Configura√ß√£o do Modelo

Para usar um modelo diferente:

1. Baixe o modelo GGUF desejado
2. Coloque em `~/suna-local/models/`
3. Edite `start-llama.sh` para apontar para o novo modelo
4. Ajuste `MODEL_FILE` em `backend/.env`

### Configura√ß√£o de Performance

Para otimizar performance:

```bash
# Editar start-llama.sh
python -m llama_cpp.server \
    --model ./models/seu-modelo.gguf \
    --n_threads $(nproc) \        # Usar todos os cores
    --n_ctx 4096 \                # Contexto maior
    --n_batch 512                 # Batch maior
```

## üß™ Testes

### Suite de Testes Automatizada

```bash
# Executar todos os testes
python test_suite.py

# Testar diret√≥rio espec√≠fico
python test_suite.py /caminho/para/suna-local
```

### Testes Manuais

#### 1. Testar Servidor Llama
```bash
cd ~/suna-local/backend
python test_llama_server.py
```

#### 2. Testar Backend
```bash
curl http://localhost:8080/health
```

#### 3. Testar Frontend
```bash
curl http://localhost:3000
```

## üõ†Ô∏è Solu√ß√£o de Problemas

### Problemas Comuns

#### 1. Servidor Llama n√£o inicia
```bash
# Verificar logs
tail -f ~/suna-local/data/logs/llama.log

# Verificar modelo
ls -la ~/suna-local/models/

# Testar manualmente
cd ~/suna-local
source venv/bin/activate
python -m llama_cpp.server --model ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf --host 0.0.0.0 --port 8000
```

#### 2. Backend n√£o conecta ao Llama
```bash
# Verificar se Llama est√° rodando
curl http://localhost:8000/v1/models

# Verificar logs do backend
tail -f ~/suna-local/data/logs/backend.log
```

#### 3. Frontend n√£o carrega
```bash
# Verificar se backend est√° rodando
curl http://localhost:8080/health

# Verificar logs do frontend
cd ~/suna-local/frontend
npm run dev
```

#### 4. Erro de mem√≥ria
```bash
# Reduzir contexto do modelo
# Editar start-llama.sh: --n_ctx 2048

# Usar modelo menor
# Baixar vers√£o Q2_K em vez de Q4_K_M
```

### Logs e Diagn√≥stico

```bash
# Ver todos os logs
tail -f ~/suna-local/data/logs/*.log

# Verificar processos
ps aux | grep -E "(llama|uvicorn|node)"

# Verificar portas
netstat -tlnp | grep -E "(8000|8080|3000|6379)"

# Testar Redis
redis-cli ping
```

## üîÑ Controle do Sistema

### Scripts de Controle

```bash
# Iniciar todos os servi√ßos
./start-suna.sh

# Parar todos os servi√ßos
./stop-suna.sh

# Iniciar servi√ßos individuais
./start-llama.sh      # Apenas Llama
./start-backend.sh    # Apenas Backend
./start-frontend.sh   # Apenas Frontend
./start-redis.sh      # Apenas Redis
```

### Monitoramento

```bash
# Verificar status dos servi√ßos
ps aux | grep -E "(llama_cpp|uvicorn|node|redis)"

# Verificar uso de recursos
htop

# Verificar logs em tempo real
tail -f ~/suna-local/data/logs/llama.log
tail -f ~/suna-local/data/logs/backend.log
```

## üîß Desenvolvimento

### Modificar o Backend

```bash
cd ~/suna-local/backend
source ../venv/bin/activate

# Editar c√≥digo
nano agent/run.py

# Reiniciar backend
pkill -f "uvicorn.*api:app"
./start-backend.sh
```

### Modificar o Frontend

```bash
cd ~/suna-local/frontend

# Editar c√≥digo
nano src/pages/index.tsx

# O frontend recarrega automaticamente em modo dev
```

### Adicionar Novas Ferramentas

1. Criar nova ferramenta em `backend/agent/tools/`
2. Registrar em `backend/agent/run.py`
3. Testar com `python test_suite.py`

## üìä Performance e Otimiza√ß√£o

### Benchmarks T√≠picos

| Componente | Tempo de Inicializa√ß√£o | Uso de RAM | Uso de CPU |
|------------|----------------------|------------|------------|
| Llama Server | 30-60s | 4-8GB | 50-100% |
| Backend | 5-10s | 200-500MB | 5-15% |
| Frontend | 10-20s | 100-300MB | 5-10% |
| Redis | 1-2s | 50-100MB | 1-5% |

### Otimiza√ß√µes

#### Para Sistemas com Pouca RAM
```bash
# Usar modelo menor
wget -O ~/suna-local/models/mistral-7b-q2.gguf \
  "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf"

# Reduzir contexto
# Editar start-llama.sh: --n_ctx 2048
```

#### Para Sistemas com Muitos Cores
```bash
# Aumentar threads
# Editar start-llama.sh: --n_threads 16

# Aumentar batch size
# Editar start-llama.sh: --n_batch 1024
```

## üîê Seguran√ßa

### Configura√ß√µes de Seguran√ßa

- **Autentica√ß√£o local**: Sistema de tokens JWT local
- **Isolamento**: Todos os servi√ßos rodam localmente
- **Dados**: Todos os dados ficam no sistema local
- **Rede**: Sem comunica√ß√£o externa necess√°ria

### Backup e Recupera√ß√£o

```bash
# Backup dos dados
tar -czf suna-backup-$(date +%Y%m%d).tar.gz ~/suna-local/data/

# Backup da configura√ß√£o
cp ~/suna-local/backend/.env ~/suna-local/backend/.env.backup
cp ~/suna-local/frontend/.env.local ~/suna-local/frontend/.env.local.backup

# Restaurar dados
tar -xzf suna-backup-YYYYMMDD.tar.gz -C ~/
```

## üÜï Atualiza√ß√µes

### Atualizar o Sistema

```bash
# Parar servi√ßos
./stop-suna.sh

# Fazer backup
tar -czf suna-backup-$(date +%Y%m%d).tar.gz ~/suna-local/data/

# Atualizar c√≥digo
git pull origin main

# Aplicar patches
python backend-patches/apply_patches.py ~/suna-local/backend ./backend-patches/

# Reinstalar depend√™ncias se necess√°rio
cd ~/suna-local/backend
source ../venv/bin/activate
pip install -r requirements.txt

cd ../frontend
npm install

# Reiniciar servi√ßos
./start-suna.sh
```

## ü§ù Contribui√ß√£o

### Reportar Problemas

1. Execute `python test_suite.py` e inclua os resultados
2. Inclua logs relevantes de `~/suna-local/data/logs/`
3. Descreva o ambiente (WSL2, Ubuntu version, hardware)

### Desenvolvimento

1. Fork o reposit√≥rio
2. Crie uma branch para sua feature
3. Teste com `python test_suite.py`
4. Submeta um pull request

## üìÑ Licen√ßa

Este projeto √© licenciado sob a Licen√ßa MIT - veja o arquivo LICENSE para detalhes.

## üôè Agradecimentos

- [SUNA AI](https://github.com/kortix-ai/suna) - Framework original
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Infer√™ncia eficiente de modelos LLaMA
- [Mistral AI](https://mistral.ai/) - Modelo Mistral 7B
- [FAISS](https://github.com/facebookresearch/faiss) - Biblioteca de busca vetorial

## üìû Suporte

Para suporte t√©cnico:

1. Consulte a se√ß√£o de Solu√ß√£o de Problemas
2. Execute a suite de testes: `python test_suite.py`
3. Verifique os logs em `~/suna-local/data/logs/`
4. Abra uma issue no reposit√≥rio com informa√ß√µes detalhadas

---

**Desenvolvido por Manus AI** - Adapta√ß√£o local do SUNA para WSL2

